<!DOCTYPE html>
<html>

<head lang="en">
    <title>Affection</title>

    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">    
    <meta name="description" content="Affection, Affection-dataset, Emotions, Deep Learning, neural-speakers">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <!--Open Graph Related Stuff-->
    <meta property="og:title" content="Affection: Learning Affective Explanations for Real-World Visual Data"/>
    <meta property="og:url" content="https://affective-explanations.org/"/>
    <meta property="og:description" content="Or, how to make AI that explains emotions grounded in images!"/>
    <meta property="og:site_name" content="Affection: Learning Affective Explanations for Real-World Visual Data"/>  
    <meta property="og:image" content="https://affective-explanations.org/img/page_logo.jpg"/>

    <!--Twitter Card Stuff-->
    <meta name="twitter:card" content="https://affective-explanations.org/img/page_logo.jpg"/>
    <meta name="twitter:title" content="Affection: Learning Affective Explanations for Real-World Visual Data"/>
    <meta name="twitter:image" content="https://affective-explanations.org/img/page_logo.jpg">
    <meta name="twitter:url" content="https://affective-explanations.org"/>
    <meta name="twitter:description" content="Or, how to make AI that explains emotions grounded in images!"/>
   

    <link rel="icon" type="image/png" href="img/favicon.png">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>


<body>

<div class="container" id="main">
    <div class="row">

        <h2 class="col-md-12 text-center" style="padding-bottom:20px">
            <img src="img/page_logo.png" height="150">
    <span style="font-size:35pt"><br> Learning Affective Explanations for<br>
Real-World Visual Data</b></span>
        </h2>
    </div>


    <div class="row" id="authors">

        <div class="col-md-12 text-center">
            <ul class="list-inline" style="font-size:17pt;">
                <li>
                    <a href="https://optas.github.io/">
                        Panos Achlioptas<br></a>                    
                        <div style ="font-size:12pt; color: black">Snap Inc.</div>                    
                </li>
                <li>
                    <a href="http://www.lix.polytechnique.fr/~maks/">
                        Maks Ovsjanikov
                    </a>
                    <div style ="font-size:12pt; color: black">École Polytechnique</div>                    
                </li>                                
                <li>
                    <a href="https://geometry.stanford.edu/member/guibas">
                        Leonidas J. Guibas
                    </a>
                    <div style ="font-size:12pt; color: black">Stanford University</div>
                </li>
                <li>
                    <a href="http://www.stulyakov.com/">
                        Sergey Tulyuakov
                    </a>
                    <div style ="font-size:12pt; color: black">Snap Inc.</div>
                </li>                
            </ul>
                    
        </div>
    </div>
 
    <div class="row" style="padding-top:40px">
        <div class="col-md-8 col-md-offset-2 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://arxiv.org/abs/2210.01946" class="imageLink">
                        <img src="img/Doc.png"  height="80">
                        <h4><strong>[Paper]</strong></h4>
                    </a>
                </li>

                

                <li>
                    <a href="#dataset">
                        <img src="img/dataset_img.png" height="80">
                        <h4><strong>[Dataset]</strong></h4>
                    </a>
                </li>

                <!-- <li> -->
                    <!-- <a href="#videos"> -->
                        <!-- <img src="img/video_.png"  width="80" height="80"> -->
                        <!-- <h4><strong>[Videos]</strong></h4> -->
                    <!-- </a> -->
                <!-- </li> -->

                <li>
                    <a href="#code">
                        <img src="img/Git.png" height="80">
                        <h4><strong>[Code] <br>(coming soon)</strong></h4>
                    </a>
                </li>

                <li>
                    <a href="https://affectivetools.github.io/materials/affection_supp_mat.pdf" class="imageLink">
                        <img src="img/SupMat.png" height="80">
                        <h4><strong>[Supplemental Material]</strong></h4>
                    </a>
                </li>

                
            </ul>
        </div>
    </div>



    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Abstract</b>
            </h3>
            <p class="text-justify"> 
                Real-world images often convey emotional intent, i.e., the photographer tries to capture and promote
                an emotionally interesting story. In this work, we explore the emotional reactions that real-world
                images tend to induce by using natural language as the medium to express the rationale behind
                an affective response to a given visual stimulus. To embark on this journey, we introduce and
                share with the research community a large-scale dataset that contains emotional reactions and free-form
                textual explanations for <b>85K</b> publicly available images, analyzed by 6,283 annotators who were
                asked to indicate and explain <b>how and why</b> they felt in a particular way when observing a particular
                image, producing a total of <b>526K</b> responses. Even though emotional reactions are subjective
                and sensitive to context (personal mood, social status, past experiences) – we show that there is
                significant common ground to capture potentially plausible emotional responses with large support in
                the subject population. In light of this key observation, we ask the following questions: i) Can we
                develop multi-modal neural networks that provide reasonable affective responses to real-world visual
                data, explained with language? ii) Can we steer such methods towards creating explanations with 
                varying degrees of pragmatic language or justifying different emotional reactions while adapting to the underlying visual stimulus?
                Finally, iii) How can we evaluate the performance of such methods for this novel task? With this
                work, we take the first steps to partially address all of these questions, thus paving the way for richer,
                more human-centric, and emotionally-aware image analysis systems. 
                
            </p>  
        </div>
    </div>


    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Summary of Main Contributions and Findings</b>
            </h3>
            <p class="text-justify"> 
                <ol>                                        

                    <li>We curate and share <b>Affection</b> a large-scale dataset with emotional reactions to <b>real-world</b> images and explanations behind them in free-form language.</li><br>
                    <li>We introduce the task of <b><u>E</u>motional <u>E</u>xplanation <u>C</u>aptioning</b> (<b>EAC</b>) and develop correspondingly <i>neural speakers</i> that can create plausible utterances to explain emotions grounded in real-world images.</li><br>
                    <li>Our best neural speaker passes an <b>emotional Turing test</b> with a 65% chance. I.e., its generations are this likely to be thought of as if other humans made them, and not machines, as judged by third-party observing humans.</li><br>
                    <li>Our developed <i>neural listeners</i> and CLIP-based studies indicate that Affection contains significant amounts of discriminative references that enable the identification of its underlying images from the affective explanations. Tapping on this observation, we also experiment and provide <b>pragmatic</b> neural speaking variants.
                </ol>   
                
            </p>  
        </div>
    </div>
    

    <div class="row" id="qualitative_results" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2" >
            <h3 style="margin-bottom:30px">
                <b>Qualitative Νeural Speaking Results</b>
            </h3>

                <!-- <div class="zoom"> -->
                <figure style="padding-bottom:80px">
                <img src="img/generations_teaser_emo_pragma_psych_oriented.jpg" class="img-responsive" style="padding-bottom:10px;" alt="overview">
                <figcaption style="background-color:#FFECF6;padding:10px">
                    <b><font color="black">Examples of neural speaker generations on unseen images from our emotion-grounded, pragmatic speaker variant.</b> The top row includes generations that reflect a positive sentiment, while the bottom row showcases generations grounded on <i>similar visual subjects</i> (object classes) e.g., another dog, food item, etc., that give rise to negative emotions. Remarkably, this neural speaker appears to take into account the underlying fine-grained visual differences to properly modulate its output, providing strong explanatory power behind the emotional reactions. Note, also, how the explanations can include purely human-centric semantics (<i>‘nostalgic of my childhood’</i>, <i>‘love coffee’</i>), and use explicit psychological assessments (<i>‘feel content/excited/disgusted’</i>, <i>‘is depressing’</i>). </font>                    
                </figcaption>
                </figure>
                <!-- </div> -->

                <!-- <div class="zoom"> -->
                <figure style="padding-bottom:80px">                                        
                    <img src="img/emo_grounded_generations_gallery_img.jpg" style="padding-bottom:0px" class="img-responsive">
                    <img src="img/emo_grounded_generations_gallery_txt.png" style="padding-bottom:15px" class="img-responsive">


                    <figcaption style="background-color:#FFECF6;padding:10px">
                        <b> <font color="black">Examples of neural speaker generations with an <i>emotion-grounded</i> speaker variant on
<i>unseen</i> test images.</b> The grounding emotion (shown in boldface fonts) is predicted during inference time by a separately trained
image-to-emotion classifier. We ground the speaker’s generation with two emotions for each image, corresponding
to the most likely (top row) and second most likely (bottom row) predictions. As seen this figure, this variant provides a certain
control over the output by aligning it to the requested/input emotion. </font>
                    </figcaption>
                </figure>
            <!-- </div> -->
                
                <!-- <div class="zoom"> -->
                <figure style="padding-bottom:80px;">                    
                    <img src="img/effect_of_pragmatics_img.jpg" style="padding-bottom:0px;" class="img-responsive">
                    <img src="img/effect_of_pragmatics_txt.png" style="padding-bottom:15px;" class="img-responsive">
                    
                    <figcaption style="background-color:#FFECF6;padding:10px">
                        <font color="black"><b>Effect of boosting the pragmatic content of neural speaker generations via <a href="https://github.com/openai/CLIP" style="color:#FF42A1">CLIP</a>.</b> Aside from often correcting the identity of shown objects/actions (right-most image is indeed taken inside an airport), this pragmatic variant tends to use more visual details in its explanations (<i>‘standing in the sand’</i>), and perhaps more importantly to expand the explanation to include non-visual but valid associations (e.g., <i>‘take a nap’</i>, or <i>‘do not like crowds’</i>). </font>                        
                    </figcaption>
                </figure>
                <!-- </div> -->


            <figcaption style="background-color:#FFECF6;padding:10px">
                <b><font color="black"><u>Legal Disclaimer</u></b>: The images shown in the above qualitative results are included in the image-centric datasets covered by our Affection dataset, and which are described in detail in the Section 3, of our <a href="https://arxiv.org/abs/2210.01946">manuscript</a>. I.e., the datasets of <a href="https://arxiv.org/abs/1405.0312">MS-COCO</a>, <a href="https://ieeexplore.ieee.org/document/8344491">Emotional-Machines</a>, <a href="https://arxiv.org/abs/1505.04870">Flickr30kEntities, <a href="https://arxiv.org/abs/1602.07332">Visual Genome</a> and those included in the work of <a href="https://arxiv.org/pdf/1605.02677.pdf">Quanzeng et al.</a> <b>We do not have or claim any ownership or copyright right for these images. Our work (Affection) is only provided for research puprposes.</b></font>
        </figcaption>

            
        </div>
    
    

    </div>




  <div class="row" id="dataset" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2" >
            <h3 style="margin-bottom:30px">
                <b>The Affection Dataset</b>
            </h3>
        <p>        
        
        <!-- You can <a href="https://forms.gle/yTdgwHPwUC4a4C8Z8">download</a> the Affection dataset upon first agreeing to its terms of use! --> 
        <!-- add link: https://forms.gle/yTdgwHPwUC4a4C8Z8 -->

        You will be able to download Affection in a few days. Please visit this page again in about a week (15th of October). 
        <!-- In the meantime, you can quickly <b>browse the dataset</b> <a href="">here</a>.</font> -->
    </p>

        <strong>Important Disclaimer:</strong> Affection is a real-world dataset containing the opinion and sentiments of thousands of people. Thus, we expect it to include text with certain biases, factual inaccuracies, and possibly foul language. The provided neural networks are also likely biased and inaccurate, similar to their training data. Their output, subjective opinions, and sentiments present in Affection are not byproducts that express the personal views and preferences of the authors by any means. Please use our work <a href="https://www.nature.com/articles/s41599-020-0501-9">responsibly</a>.
    
        </div>
    </div>

       

    <div class="row" id="contact" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Contact</b>
            </h3>            
            To contact all authors, please use <i>affective.explanations@gmail.com</i>, or their individual <a href="#authors"> emails</a>.

        </div>
    </div>
    

    <div class="row" id="citation" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Citation</b>
             </h3>
            <p class="text-justify">If you find our work useful in your research, please consider citing:</p>

            <pre class="w1-panel w3-leftbar w3-light-grey">
@article{achlioptas2022affection,
    title={{Affection}: Learning Affective Explanations for
                      Real-World Visual Data},
    author={Achlioptas, Panos and Ovsjanikov, Maks 
            and Guibas, Leonidas and Tulyakov, Sergey},
    journal={Computing Research Repository (CoRR)},
    volume={abs/2210.01946},
    year={2022}}</pre>        
        </div>
    </div>

    

    <div class="row" style="padding-bottom:30px">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                <b>Acknowledgements</b>
            </h3>
            <p class="text-justify">                
            P.A. wants to thank Professors <a href="https://spl.stanford.edu/people/james-gross"> James Gross</a>, <a href="http://cocolab.stanford.edu/ndg">
                        Noah Goodman
                    </a> and <a href="https://web.stanford.edu/~jurafsky/"> Dan Jurafsky</a> for their initial discussions and the ample motivation they provided for exploring this research direction. Also, wants to thank <a href="https://psychology.stanford.edu/people/ashish-mehta"> Ashish Mehta</a> for fruitful discussions on alexithymia, and <a href="https://www.linkedin.com/in/gregorykozhemyak/">Grygorii Kozhemiak</a> for helping design Affection's logos and webpage. Last but not least, the authors want to emphasize their gratitude to all the hard-working Amazon Mechanical Turkers without whom this work would be impossible.
            <br><br>
            Parts of this work were supported by the ERC Starting Grant No. 758800 (EXPROTEA), the ANR AI Chair AIGRETTE, and a Vannevar Bush Faculty Fellowship.
            <br>
            
            </p>
        </div>
    </div>    

    <div class="row" style="padding-top:40px">
        <div class="col-md-6 col-md-offset-3">    
            <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=a&t=n&d=pw5OwRMar-gbF4_DR9iwaLozbgYvAItD2etl8V25T4c&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>            
        </div>
    </div>

<!-- </div> -->




</body>


<!-- Panos Added Style Stuff -->

<style>
.zoom {
  padding: 50px;  
  transition: transform .1.7s; /* Animation */  
  margin: 0 auto;
}

.zoom:hover {
  transform: scale(1.75); /* (150% zoom - Note: if the zoom is too large, it will go outside of the viewport) */
}

</style>


<style>
a:link, a:visited {  
  color: #FF42A1;  
}

</style>

</html>
